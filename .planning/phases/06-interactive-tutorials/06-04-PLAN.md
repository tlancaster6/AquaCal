---
phase: 06-interactive-tutorials
plan: 04
type: execute
wave: 2
depends_on: ["06-02"]
files_modified:
  - docs/tutorials/02_diagnostics.ipynb
  - docs/tutorials/03_synthetic_validation.ipynb
autonomous: true

must_haves:
  truths:
    - "User can run diagnostics notebook to visualize calibration results and interpret quality metrics"
    - "Diagnostics notebook covers reprojection error analysis, parameter convergence, and 3D rig visualization"
    - "User can run synthetic validation notebook to compare refractive vs non-refractive calibration"
    - "Synthetic validation notebook shows users first-hand when refractive model matters vs when approximation is good enough"
    - "Both notebooks have data source toggle, Colab badge, and inline failure mode callouts"
    - "All notebooks execute end-to-end without manual data preparation"
  artifacts:
    - path: "docs/tutorials/02_diagnostics.ipynb"
      provides: "Calibration diagnostics and visualization tutorial"
    - path: "docs/tutorials/03_synthetic_validation.ipynb"
      provides: "Refractive vs non-refractive comparison tutorial"
  key_links:
    - from: "docs/tutorials/02_diagnostics.ipynb"
      to: "aquacal.CalibrationResult"
      via: "load_calibration or pipeline result"
      pattern: "CalibrationResult"
    - from: "docs/tutorials/03_synthetic_validation.ipynb"
      to: "tests/synthetic/experiments.py"
      via: "interactive extension of experiment patterns"
      pattern: "n_water.*1\\.0"
---

<objective>
Create the diagnostics and synthetic validation tutorial notebooks, completing the full set of three interactive tutorials.

Purpose: Satisfy NB-02 (diagnostics visualization), NB-03 (synthetic validation), and TUT-03 (failure modes and diagnostics interpretation).
Output: Pre-executed `docs/tutorials/02_diagnostics.ipynb` and `docs/tutorials/03_synthetic_validation.ipynb`.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/06-interactive-tutorials/06-02-SUMMARY.md
@src/aquacal/datasets/__init__.py
@src/aquacal/config/schema.py
@tests/synthetic/experiments.py
@tests/synthetic/experiment_helpers.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create diagnostics and visualization notebook</name>
  <files>docs/tutorials/02_diagnostics.ipynb</files>
  <action>
    Create `docs/tutorials/02_diagnostics.ipynb` -- self-contained notebook for interpreting calibration results.

    **Cell structure:**

    1. **Title + Colab badge** (markdown):
       - Title: "Calibration Diagnostics and Visualization"
       - Colab badge linking to this notebook on GitHub main branch
       - Intro: what this tutorial covers (understanding calibration quality)

    2. **Data source toggle** (code): same pattern as 01_full_pipeline.ipynb

    3. **Setup** (code):
       - Import aquacal, numpy, matplotlib
       - Load data and run calibration (or load a saved result)
       - Use synthetic "small" preset by default for speed

    4. **Reprojection Error Analysis** (markdown + code):
       - Explain what reprojection error means and what good values look like
       - Per-camera RMS error bar chart
       - Per-frame error distribution (histogram)
       - Error heatmap: spatial distribution of errors across image plane (scatter plot of corner positions colored by error magnitude)
       - **Warning callout**: "High error in one camera often indicates poor intrinsics or insufficient board observations for that camera"

    5. **Parameter Convergence** (markdown + code):
       - Show optimization cost function over iterations
       - Interface distance convergence: plot estimated vs initial values
       - For synthetic data: compare estimated parameters to ground truth
       - **Warning callout**: "If interface distance diverges or oscillates, the initial estimate may be too far from truth. Try providing explicit initial_distances in config."

    6. **3D Rig Visualization** (markdown + code):
       - 3D matplotlib plot showing:
         - Camera positions as markers with orientation arrows
         - Water surface plane
         - Board positions (if available)
         - Camera frustums (simplified)
       - Multiple viewing angles
       - For synthetic: overlay ground truth camera positions for comparison

    7. **Validation Metrics** (markdown + code):
       - Holdout vs training reprojection error comparison
       - 3D reconstruction error (triangulated board corner spacing vs known)
       - Statistical summary table

    8. **Common Issues Checklist** (markdown):
       - Table of symptoms, likely causes, and fixes
       - E.g., "High error for one camera" -> "Check intrinsic calibration quality"
       - "Interface distance not converging" -> "Provide better initial estimate"
       - "Degenerate board poses" -> "Ensure board is visible at varied angles"

    **Key requirements per user decisions:**
    - Covers reprojection error analysis, parameter convergence, AND 3D rig visualization
    - Rich visualizations: error heatmaps, 3D plots, before/after comparisons
    - Failure modes as inline warning callouts
    - Self-contained, works with synthetic data by default
    - Pre-executed with committed outputs
    - Close figures with plt.close() after display
  </action>
  <verify>
    ```bash
    python -c "import json; nb=json.load(open('docs/tutorials/02_diagnostics.ipynb')); print(f'Cells: {len(nb[\"cells\"])}')"
    ```
  </verify>
  <done>Diagnostics notebook exists with reprojection error analysis, parameter convergence plots, 3D rig visualization, and failure mode callouts.</done>
</task>

<task type="auto">
  <name>Task 2: Create synthetic validation notebook</name>
  <files>docs/tutorials/03_synthetic_validation.ipynb</files>
  <action>
    Create `docs/tutorials/03_synthetic_validation.ipynb` -- interactive extension of tests/synthetic/experiments.py showing why refractive calibration matters.

    **Cell structure:**

    1. **Title + Colab badge** (markdown):
       - Title: "Synthetic Validation: Why Refractive Calibration Matters"
       - Colab badge
       - Intro: comparing refractive (n_water=1.333) vs non-refractive (n_water=1.0) calibration

    2. **Data source toggle** (code): synthetic only for this notebook (toggle selects rig size)
       ```python
       RIG_SIZE = "small"  # Options: "small" (fast), "medium", "large"
       ```

    3. **Setup** (code):
       - Import aquacal, numpy, matplotlib
       - Generate synthetic scenario with known ground truth
       - Explain the experimental approach: run calibration twice -- once with correct refractive model, once with n_water=1.0 (no refraction)

    4. **Generate Ground Truth** (markdown + code):
       - Create synthetic rig with generate_synthetic_rig()
       - Show the ground truth parameters: camera positions, interface distances, board poses
       - Visualize the ground truth rig in 3D

    5. **Experiment: Refractive vs Non-Refractive** (markdown + code):
       - Run calibration with n_water=1.333 (correct refractive model)
       - Run calibration with n_water=1.0 (non-refractive approximation)
       - This mirrors the approach in tests/synthetic/experiments.py

    6. **Reprojection Error Comparison** (markdown + code):
       - Side-by-side bar charts: per-camera RMS for refractive vs non-refractive
       - Overall RMS comparison
       - Explain: non-refractive model forces optimizer to absorb refraction errors into other parameters

    7. **Parameter Recovery Comparison** (markdown + code):
       - Compare recovered extrinsics to ground truth for both models
       - Interface distance recovery: refractive model estimates interface distance correctly; non-refractive cannot
       - Bar charts or scatter plots showing parameter errors
       - **Warning callout**: "Even if non-refractive RMS looks acceptable, the recovered parameters are systematically biased -- they don't correspond to physical camera positions"

    8. **3D Reconstruction Quality** (markdown + code):
       - Triangulate known board corners using both calibrations
       - Compare triangulated positions to ground truth
       - Show 3D reconstruction error is much larger for non-refractive
       - Error distribution histogram

    9. **When Does It Matter?** (markdown + code):
       - Explore: how does error scale with interface distance?
       - Small interface distance -> small refraction -> non-refractive is "close enough"
       - Large interface distance -> significant refraction -> refractive model is essential
       - Plot: reconstruction error vs interface distance for both models

    10. **Summary** (markdown):
        - Key takeaway: refractive model is essential when camera-to-water distance is significant relative to working volume
        - Link back to theory documentation for mathematical details

    **Key requirements per user decisions:**
    - Focus on refractive vs non-refractive comparison (set n_air=n_water=1.0)
    - Interactive extension of tests/synthetic/experiments.py
    - Shows users first-hand why refractive calibration matters
    - Rich visualizations: side-by-side comparisons, 3D plots, error distributions
    - Self-contained with synthetic data (no download needed)
    - Pre-executed with committed outputs
    - Close figures with plt.close()

    **Implementation approach:**
    - Adapt the logic from tests/synthetic/experiments.py and experiment_helpers.py
    - Use aquacal.datasets.generate_synthetic_rig() for data generation
    - Run calibration pipeline programmatically (not via CLI)
    - For the non-refractive comparison: modify the config to set n_water=1.0 (or n_air=n_water=1.0)
  </action>
  <verify>
    ```bash
    python -c "import json; nb=json.load(open('docs/tutorials/03_synthetic_validation.ipynb')); print(f'Cells: {len(nb[\"cells\"])}')"
    ```
  </verify>
  <done>Synthetic validation notebook exists with refractive vs non-refractive comparison, parameter recovery analysis, reconstruction quality metrics, and interface distance sensitivity exploration.</done>
</task>

</tasks>

<verification>
```bash
# Both notebooks are valid JSON
python -c "import json; json.load(open('docs/tutorials/02_diagnostics.ipynb')); print('02 valid')"
python -c "import json; json.load(open('docs/tutorials/03_synthetic_validation.ipynb')); print('03 valid')"

# Sphinx build with all notebooks
cd docs && python -m sphinx -b html . _build/html 2>&1 | tail -5

# All three notebooks exist
ls docs/tutorials/*.ipynb
```
</verification>

<success_criteria>
- Diagnostics notebook exists with reprojection error analysis, convergence plots, 3D rig visualization, and common issues checklist
- Synthetic validation notebook exists with refractive vs non-refractive comparison across parameter recovery, reprojection error, and reconstruction quality
- Both notebooks have data source toggle, Colab badge, and inline warning callouts
- Both notebooks work with synthetic data by default (no download)
- Sphinx builds and renders all three notebooks as HTML pages
- All notebooks are pre-executed with committed outputs (or structured for execution)
</success_criteria>

<output>
After completion, create `.planning/phases/06-interactive-tutorials/06-04-SUMMARY.md`
</output>
