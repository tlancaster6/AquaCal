---
phase: 12-tutorial-verification
plan: 02
type: execute
wave: 2
depends_on: ["12-01"]
files_modified:
  - docs/tutorials/02_synthetic_validation.ipynb
  - tests/synthetic/test_refractive_comparison.py
autonomous: true
must_haves:
  truths:
    - "Tutorial 02 contains three progressive experiments: parameter fidelity, depth generalization, depth scaling"
    - "Each experiment builds on the previous with a narrative arc"
    - "RIG_SIZE toggle with small and large presets works"
    - "All required plots per brief are present; excluded plots (distortion, XY heatmaps, CSV, redundant Z error) are absent"
    - "test_refractive_comparison.py is deleted"
  artifacts:
    - path: "docs/tutorials/02_synthetic_validation.ipynb"
      provides: "Complete synthetic validation tutorial with 3 experiments"
      contains: "run_experiment"
    - path: "tests/synthetic/experiment_helpers.py"
      provides: "Shared experiment utilities (kept, not deleted)"
    - path: "tests/synthetic/experiments.py"
      provides: "Experiment logic (kept, not deleted)"
  key_links:
    - from: "docs/tutorials/02_synthetic_validation.ipynb"
      to: "tests/synthetic/experiment_helpers.py"
      via: "import statement"
      pattern: "from tests.synthetic.experiment_helpers import"
    - from: "docs/tutorials/02_synthetic_validation.ipynb"
      to: "tests/synthetic/experiments.py"
      via: "experiment function calls or adapted logic"
      pattern: "calibrate_synthetic|compute_per_camera_errors|evaluate_reconstruction"
---

<objective>
Rewrite tutorial 02 (synthetic validation) with three progressive experiments from the conversion brief, delete the old test wrappers, and execute both tutorials to verify they run without errors.

Purpose: Transform experiment demonstrations into a cohesive teaching narrative showing why refractive calibration matters.
Output: Complete, pre-executed tutorial 02 notebook; verified tutorial 01; deleted test file.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/experiment-to-tutorial-brief.md
@.planning/phases/12-tutorial-verification/12-01-SUMMARY.md
@tests/synthetic/experiments.py
@tests/synthetic/experiment_helpers.py
@docs/tutorials/02_synthetic_validation.ipynb
</context>

<tasks>

<task type="auto">
  <name>Task 1: Rewrite tutorial 02 with three progressive experiments</name>
  <files>
    docs/tutorials/02_synthetic_validation.ipynb
    tests/synthetic/test_refractive_comparison.py
  </files>
  <action>
Replace the entire content of `docs/tutorials/02_synthetic_validation.ipynb` with a new notebook following the conversion brief. The notebook has a progressive narrative: "refraction matters" then "it matters more at depth" then "here's how accuracy scales".

**Structure:**

1. **Title + intro markdown cell**: "Why Refractive Calibration Matters" (or similar inviting title). Colab badge pointing to `tlancaster6/AquaCal`. Brief overview: three experiments that progressively reveal the importance of refractive modeling.

2. **RIG_SIZE toggle cell**:
   ```python
   RIG_SIZE = "small"  # "small" (4 cameras, 20 frames, fast) or "large" (13 cameras, 30 frames, compelling results)
   ```

3. **Setup + imports cell**: Import from `tests.synthetic.experiment_helpers` (calibrate_synthetic, compute_per_camera_errors, evaluate_reconstruction), from `tests.synthetic.ground_truth` (create_scenario, generate_real_rig_array, generate_real_rig_trajectory, generate_dense_xy_grid, generate_synthetic_detections, SyntheticScenario), numpy, matplotlib. Also import BoardConfig, BoardGeometry, InterfaceParams.

4. **Preset configuration cell**: Define presets based on RIG_SIZE. **IMPORTANT API note**: `create_scenario()` in `tests/synthetic/ground_truth.py` accepts ONLY "ideal", "minimal", or "realistic" — there is NO "small" preset. `generate_synthetic_rig()` in `aquacal.datasets.synthetic` has "small" (2 cameras), "medium" (6 cameras), "large" (13 cameras) — but "small" is only 2 cameras, not 4.
   - "small": use `create_scenario("ideal")` which gives 4 cameras, 20 frames, 0 noise. Use 3 test depths for exp2/3.
   - "large": use `create_scenario("realistic")` which gives 13 cameras, 30 frames, 0.5px noise. Use full depth sweeps.
   Print the active configuration.

5. **Experiment 1: Parameter Fidelity** (4-6 cells)
   - Markdown intro explaining the experiment: calibrate refractive vs non-refractive on same data
   - Code cell: generate scenario + calibrate both models (adapt from `run_experiment_1()` in experiments.py)
   - **Plot: Focal length error** (grouped bar chart, refractive vs non-refractive per camera)
   - **Plot: Camera Z position error** (grouped bar chart per camera)
   - **Plot: XY position scatter** (ground truth + refractive + non-refractive camera positions with arrows)
   - Markdown narrative: "Both models achieve low reprojection error, but non-refractive absorbs refraction into focal length and position bias"
   - Do NOT include: distortion plot (all zeros), CSV output

6. **Experiment 2: Depth Generalization** (4-6 cells)
   - Markdown intro: "What happens when we test at depths we didn't calibrate at?"
   - Code cell: use the scenario from the preset (4-camera "ideal" for small, 13-camera "realistic" for large), calibrate on narrow depth band (0.95-1.05m), evaluate at multiple test depths. For "small" preset use 3 test depths (e.g., 0.85, 1.0, 1.5); for "large" use full sweep [0.80, 0.90, 1.00, 1.10, 1.20, 1.40, 1.70, 2.00].
   - **Plot: Signed error vs depth** (line plot with calibration range shaded)
   - **Plot: RMSE vs depth** (line plot with calibration range shaded)
   - **Plot: Scale factor vs depth** (line plot showing measured/true ratio)
   - Markdown narrative: "Non-refractive error grows with depth because the wrong geometry can't generalize"
   - Do NOT include: XY heatmaps, CSV output

7. **Experiment 3: Depth Scaling** (4-6 cells)
   - Markdown intro: "Does calibrating at the right depth help the non-refractive model?"
   - Code cell: for each depth, calibrate at that depth and evaluate at same depth. For "small" use 3 depths; for "large" use [0.85, 1.0, 1.2, 1.5, 2.0, 2.5].
   - **Plot: RMSE vs depth** (line plot showing both models)
   - **Plot: Focal length error vs depth** (line plot)
   - Markdown narrative: "Even with matched depth, non-refractive error scales because the geometry is fundamentally wrong"
   - Do NOT include: Z error plot, XY heatmaps, CSV output

8. **Optional: Summary visualization** (Claude's discretion) — if it strengthens the narrative, add a single summary figure comparing key metrics across all 3 experiments (e.g., a 1x3 subplot showing the "headline" result from each experiment).

9. **Summary + takeaways markdown cell**: Key findings, when refractive calibration is essential vs when pinhole might suffice, link back to tutorial 01 and theory docs.

**Important implementation details:**
- Use the `experiments.py` plotting logic as reference for plot styling (COLOR_REFRACTIVE = "#2196F3", COLOR_NON_REFRACTIVE = "#F44336") but generate plots inline (not saving to files)
- Import `calibrate_synthetic`, `compute_per_camera_errors`, `evaluate_reconstruction` from `experiment_helpers.py` — do NOT duplicate that logic
- For experiment 2 and 3, adapt the rig generation and depth sweep logic from `run_experiment_2()` and `run_experiment_3()` in experiments.py but inline it (don't call those functions directly since they save files and have hardcoded params)
- All plots use `plt.show()` then `plt.close()` pattern
- Clear all outputs initially (will be pre-executed in Task 2)

After writing the notebook, delete `tests/synthetic/test_refractive_comparison.py`:
```bash
git rm tests/synthetic/test_refractive_comparison.py
```
  </action>
  <verify>
- `docs/tutorials/02_synthetic_validation.ipynb` is valid JSON and parseable as notebook
- Contains cells for all 3 experiments with correct plots per brief
- Imports from `experiment_helpers.py` (not duplicating calibration logic)
- Has RIG_SIZE toggle cell
- Preset config uses `create_scenario("ideal")` for small and `create_scenario("realistic")` for large — NOT `create_scenario("small")` or `generate_synthetic_rig("small")`
- `tests/synthetic/test_refractive_comparison.py` does NOT exist
- `tests/synthetic/experiments.py` and `tests/synthetic/experiment_helpers.py` still exist
  </verify>
  <done>Tutorial 02 is fully rewritten with 3 progressive experiments, test wrapper file is deleted</done>
</task>

<task type="auto">
  <name>Task 2: Execute both tutorials and verify clean runs</name>
  <files>
    docs/tutorials/01_full_pipeline.ipynb
    docs/tutorials/02_synthetic_validation.ipynb
  </files>
  <action>
Execute both tutorials from scratch to verify they run without errors, then commit the pre-executed versions.

**Step 1: Execute tutorial 01**
```bash
cd C:/Users/tucke/PycharmProjects/AquaCal
jupyter nbconvert --to notebook --execute --ExecutePreprocessor.timeout=300 docs/tutorials/01_full_pipeline.ipynb --output 01_full_pipeline.ipynb
```
If execution fails, diagnose and fix the notebook (likely import paths or API changes from the merge).

**Step 2: Execute tutorial 02 with small preset first (quick validation)**
```bash
jupyter nbconvert --to notebook --execute --ExecutePreprocessor.timeout=600 docs/tutorials/02_synthetic_validation.ipynb --output 02_synthetic_validation.ipynb
```
The small preset uses `create_scenario("ideal")` (4 cameras, 20 frames) and should complete quickly. If it fails, fix the notebook.

**Step 3: After small preset passes, update RIG_SIZE to "large" and re-execute for production outputs**
- Edit the RIG_SIZE cell to set `RIG_SIZE = "large"`
- Re-execute:
```bash
jupyter nbconvert --to notebook --execute --ExecutePreprocessor.timeout=7200 docs/tutorials/02_synthetic_validation.ipynb --output 02_synthetic_validation.ipynb
```
- This will take significant time (potentially 30-60+ minutes with 13 cameras and multiple calibrations). Use `--ExecutePreprocessor.timeout=7200` (2 hours).
- If the large preset takes too long or fails, fall back to small preset outputs and note this in the summary.

**Step 4: Verify both notebooks have populated outputs**
```python
import json
for nb_path in ['docs/tutorials/01_full_pipeline.ipynb', 'docs/tutorials/02_synthetic_validation.ipynb']:
    nb = json.load(open(nb_path))
    code_cells = [c for c in nb['cells'] if c['cell_type'] == 'code']
    cells_with_output = sum(1 for c in code_cells if c.get('outputs'))
    print(f"{nb_path}: {cells_with_output}/{len(code_cells)} code cells have outputs")
```

**Step 5: Run existing test suite to confirm nothing is broken**
```bash
python -m pytest tests/ -m "not slow" --tb=short -q
```
  </action>
  <verify>
- Both notebooks execute without errors via nbconvert
- Both notebooks have populated cell outputs
- `python -m pytest tests/ -m "not slow"` passes (no regressions from deleting test file)
- No import errors or missing module issues
  </verify>
  <done>Both tutorials execute cleanly with pre-populated outputs, test suite passes</done>
</task>

</tasks>

<verification>
- Tutorial 02 has 3 experiments matching the conversion brief
- Both tutorials execute without errors
- Tests pass (deleted test file doesn't break anything since it was @pytest.mark.slow)
- Notebooks have embedded outputs for documentation builds
</verification>

<success_criteria>
All Jupyter tutorials run end-to-end without errors. Tutorial 02 contains the progressive experiment narrative. Pre-executed outputs are embedded. Test suite still passes.
</success_criteria>

<output>
After completion, create `.planning/phases/12-tutorial-verification/12-02-SUMMARY.md`
</output>
