---
phase: "08-cli-qa-execution"
plan: "01"
type: "execute"
wave: 1
depends_on: []
files_modified: []
autonomous: false

must_haves:
  truths:
    - "User has run aquacal init with real data directories and confirmed config output is correct"
    - "User has run aquacal calibrate end-to-end with real data and confirmed output correctness"
    - "User has run aquacal compare on multiple calibration runs and validated comparison output"
    - "Any bugs or friction points discovered are documented for Phase 9 triage"
  artifacts: []
  key_links: []
---

<objective>
Guide the user through manual QA of all three CLI workflows (init, calibrate, compare) using real rig data, collecting feedback on bugs and friction points.

Purpose: This phase cannot be automated -- only the user has real rig data and can judge output correctness. The plan structures the QA session so each workflow is tested methodically and findings are captured.
Output: User confirmation of each workflow, plus a list of bugs/friction points for Phase 9.
</objective>

<execution_context>
@C:/Users/tucke/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/tucke/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@src/aquacal/cli.py
</context>

<tasks>

<task type="checkpoint:human-verify">
  <name>Task 1: QA the aquacal init workflow</name>
  <files>src/aquacal/cli.py</files>
  <action>
    User tests the init command with real rig data directories. The command generates a config YAML
    from intrinsic and extrinsic video directories by scanning for video files and extracting camera
    names via regex.

    Steps for user:
    1. Run: python -m aquacal init --intrinsic-dir path/to/intrinsic --extrinsic-dir path/to/extrinsic -o test_config.yaml
       (optionally use --pattern if filenames need a regex to extract camera names)
    2. Verify test_config.yaml: all cameras listed, paths correct, board TODOs present
    3. Try edge cases: mismatched camera names, omitting --pattern when needed
    4. Clean up: delete test_config.yaml when done
  </action>
  <verify>User reports whether init generates correct config from their real video directories</verify>
  <done>User confirms init output is correct, or documents specific bugs/friction</done>
</task>

<task type="checkpoint:human-verify">
  <name>Task 2: QA the aquacal calibrate workflow</name>
  <files>src/aquacal/cli.py</files>
  <action>
    User tests the calibrate command with a real config YAML pointing to real rig data. The command
    runs the full 4-stage pipeline (intrinsics, extrinsics, refractive BA, optional intrinsic refinement).

    Steps for user:
    1. Prepare a config YAML with correct board dimensions, video paths, and settings
    2. Dry run first: python -m aquacal calibrate config.yaml --dry-run
       - Verify it reports correct cameras, board size, output directory
    3. Full run: python -m aquacal calibrate config.yaml -v
       - Watch verbose output for warnings or errors, note runtime
    4. Check output directory: calibration.json exists, RMS reasonable (typically less than 2 px),
       camera positions make physical sense, water_z is plausible
    5. Try -o flag to override output directory
  </action>
  <verify>User reports whether calibration completes successfully and produces plausible results</verify>
  <done>User confirms calibrate works end-to-end with real data, or documents specific bugs/friction</done>
</task>

<task type="checkpoint:human-verify">
  <name>Task 3: QA the aquacal compare workflow</name>
  <files>src/aquacal/cli.py</files>
  <action>
    User tests the compare command with 2+ calibration output directories. The command generates
    comparison reports (CSVs, plots) showing metric differences, parameter drifts, and per-camera breakdowns.

    Steps for user:
    1. Need at least 2 calibration output directories (from different runs or settings).
       If only one exists, re-run calibrate with different settings to produce a second.
    2. Run: python -m aquacal compare dir1 dir2 -o comparison_output
    3. Check comparison_output/: CSVs (metrics, per-camera, parameter diffs), PNG plots
       (RMS bar chart, position overlay, depth error if spatial data available)
    4. Try --no-plots flag
    5. Try with 3+ directories if available
    6. Assess: Are comparisons meaningful? Labels clear? Plots useful?
  </action>
  <verify>User reports whether compare produces useful and correct comparison output</verify>
  <done>User confirms compare works correctly, or documents specific bugs/friction</done>
</task>

</tasks>

<verification>
All three CLI workflows have been tested with real rig data and user has reported findings.
</verification>

<success_criteria>
1. User has confirmed aquacal init generates correct config from real video directories
2. User has confirmed aquacal calibrate completes end-to-end and produces plausible results
3. User has confirmed aquacal compare produces meaningful comparison output
4. All bugs and friction points are documented (will be triaged in Phase 9)
</success_criteria>

<output>
After completion, create `.planning/phases/08-cli-qa-execution/08-01-SUMMARY.md`
</output>
